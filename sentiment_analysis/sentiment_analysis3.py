# -*- coding: utf-8 -*-
"""Sentiment Analysis3.ipynb

Automatically generated by Colaboratory.

Original file is located at
    https://colab.research.google.com/drive/1jempJxiIPaC3IsqSfWJuMCRwq-IICzF6
"""

import req_gensim
from gensim.models import word2vec
from gensim.models.word2vec import Word2Vec
import numpy as np
import matplotlib.pyplot as plt
import pandas as pd
import spacy
import string

from gensim.models import KeyedVectors
wv = KeyedVectors.load('D:/Main_Project/vectors.kv')

data = pd.read_csv("D:/Main_Project/IMDB_Dataset.csv",error_bad_lines=False, engine="python")
print(data.head())

def sent_vec(sent):
    vector_size = wv.vector_size
    wv_res = np.zeros(vector_size)
    # print(wv_res)
    ctr = 1
    for w in sent:
        if w in wv:
            ctr += 1
            wv_res += wv[w]
    wv_res = wv_res/ctr
    return wv_res

# Creating our tokenizer function
def spacy_tokenizer(sentence):
    # Creating our token object, which is used to create documents with linguistic annotations.
    doc = nlp(sentence)

    # print(doc)
    # print(type(doc))

    # Lemmatizing each token and converting each token into lowercase
    mytokens = [ word.lemma_.lower().strip() for word in doc ]

    # print(mytokens)

    # Removing stop words
    mytokens = [ word for word in mytokens if word not in stop_words and word not in punctuations ]

    # return preprocessed list of tokens
    return mytokens

nlp = spacy.load("en_core_web_sm")
# stop_words = nlp.Defaults.stop_words
# print(stop_words)
import nltk
nltk.download('stopwords')
from nltk.corpus import stopwords
stop_words = set(stopwords.words('english'))

punctuations = string.punctuation
print(punctuations)

data=data[0:3000]

def remove_html_tags(text):
    """Remove html tags from a string"""
    import re
    clean = re.compile('<.*?>')
    return re.sub(clean, '', text)

data['review']=data['review'].apply(remove_html_tags)

data['review'] = data['review'].apply(spacy_tokenizer)

print(data.head())

data['review'] = data['review'].apply(sent_vec)

print(data.tail())

from keras.preprocessing.sequence import pad_sequences
X = pad_sequences(data['review'])

X = data['review'].to_list()
y = data['sentiment'].to_list()

from sklearn.model_selection import train_test_split
X_train, X_test, y_train, y_test = train_test_split(X, y, test_size=0.2,stratify=y)

"""SVM"""

import time
from sklearn import svm
from sklearn.metrics import classification_report
# Perform classification with SVM, kernel=linear
classifier_linear = svm.SVC(kernel='poly',C=2)
t0 = time.time()
classifier_linear.fit(X_train,y_train)
t1 = time.time()
prediction_linear = classifier_linear.predict(X_test)
t2 = time.time()
time_linear_train = t1-t0
time_linear_predict = t2-t1
# results
print("Training time: %fs; Prediction time: %fs" % (time_linear_train, time_linear_predict))
report = classification_report(y_test, prediction_linear, output_dict=True)
print('positive: ', report['positive'])
print('negative: ', report['negative'])